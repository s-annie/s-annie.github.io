---
title: 【强化学习导论】k臂赌博机问题(6)・上限置信区间动作选择
date: 2021/08/14
tags: 
- 强化学习
---
$\epsilon$贪婪行动使得非贪婪行动也有了被选择的机会，但是在选择非贪婪行动的时候并不对它们进行区分，因此接近贪婪值抑或是特别不确定的行动是同等的。为了解决这个问题，考虑到他们的估计与最大值的接近程度以及不确定性的同时，根据它们实际上最优的潜力来选择。这样做的一种有效方法是根据选择行动。
<!--more-->

$$A_{t}\doteq\underset{a}{argmax}\left[Q_{t}(a)+c\sqrt\frac{\ln{t}}{N_{t}(a)}\right]$$

$\ln{t}$表示$t$的自然对数，$N_{t}(a)$表示在时间$t$之前选择动作$a$的次数，数字$c>0$控制探索的程度。如果$N_{t}(a)=0$，则$a$被认为是最大化的行动。

这种上限置信区间（Upper-Confidence-Bound, UCB）行动选择的想法是，**平方根项是对一个估计值的不确定性或方差的度量**。因此，最大化的数量是动作$a$的可能真实值的一种上限，其中$c$确定置信水平。每次选择$a$时，$N_{t}(a)$递增，不确定性可能会降低。
另一方面，每次选择除$a$之外的动作时，$t$增加但$N_{t}(a)$不增加，不确定性估计值会增加。  
使用自然对数意味着随着时间的推移，增加量会变小，但是无限制。最终将选择所有操作，但是将随着时间的推移，具有较低值估计或已经频繁选择的操作的选择频率会降低。

UCB通常表现良好，但比起$\epsilon$贪婪，UCB更难向更为普遍的强化学习环境扩展。一个难点在于处理非平稳问题，另一个难点时处理大的状态空间。在这些更高级的设置中，UCB动作选择的想法是不太实际的。
