---
layout: post
title: 【强化学习导论】有限马尔可夫决策过程
categories: 强化学习
---
本章中将介绍有限马尔可夫决策过程（即有限MDP）的形式问题。MDP是顺序决策的经典形式化，行动不仅影响直接奖励，还影响后续情况或状态，以及贯穿未来的奖励。因此MDP涉及延迟奖励以及交换即时与延迟奖励的需要。

## 个体环境接口
MDP旨在直接构建从交互中学习以实现目标的问题。学习者和决策者被称为个体(agent)，与之交互的东西被称为环境。这些交互持续不断，个体选择动作，同时环境响应那些动作并向个体呈现新情况。环境还产生奖励，这是个体通过其行动选择寻求最大化的特殊数值。

个体和环境在离散事件序列的每一步相互作用。在每个时间步$t$（$t = 0, 1, 2, 3...$），个体接受环境状态$S_{t}\in S$,并在此基础上选择一个动作$A_{t} \in S_{s}$ 。每一步后，作为它行动的结果，个体接收到了一个奖励值$R_{t+1} \in R \subset \mathbb{R}$，并且自身处于一个新状态$S_{t+1}$。MDP和个体一起产生如下所示的序列或轨迹。
$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, R_{3}...$$
在有限MDP中，状态，动作和奖励（$S$，$A$和$R$）的集合都具有有限数量的元素。在这种情况下，随机变量$R_{t}$和$S_{t}$具有明确定义的离散概率分布，仅取决于先前的状态和动作。也就是说，对于这些随机变量的特定值，$s' \in S$和$r \in R$，在给定前一状态和动作的特定值的情况下，存在这些值在时间t发生的概率。
$$p(s',r|s, a) \doteq P_{r}\{S_{t}=s', R_{t}=r|S_{t-1}=s, A_{t-1}=a\}$$

函数$p$定义了MDP的动态。方程中等号上的点提醒我们它只是一个定义（在这个例子中是函数$p$），而不是先前定义得出的事实。动力学函数$p: S \times R \times S \times A\to[0, 1]$是四个参数的普通确定性函数。中间的“|”来自条件概率的符号，这里只是提醒我们$p$指定$s$和$a$的每个选择的概率分布，即

$$\sum_{s'\in S} \sum_{r\in R} p(s',r|s,a)=1，s\in S,a\in A(s)$$

在马尔可夫决策过程中，$p$给出的概率完全表征了环境的动态。$S_{t}$和$R_{t}$的每个可能值的概率仅取决于前一个状态和动作$S_{t-1}$和$A_{t-1}$。状态必须包括有关过去的个体-环境交互的所有方面的信息，这些信息对未来有所影响。如果确实如此，那么就说该状态拥有马尔可夫性。



## 目标和奖励
在强化学习中，个体的目的或目标被形式化为从环境传递到个体的特殊信号（称为奖励）。在每个时间步骤，奖励是一个简单的数字，$R_{t} \in \mathbb{R}$。非正式地，个体的目标是最大化其收到的总奖励，这意味着最大化不是立即奖励，而是长期累积奖励。使用奖励信号来形式化目标的想法是强化学习的最显著特征之一。
尽管根据奖励信号制定目标可能最初看起来有限，但在实践中它已被证明是可行的和广泛适用的。看到这一点的最佳方法是考虑如何使用或可能使用它的示例。

## 回报和情节
如果将时间步骤t之后接受的奖励序列表示为$R_{t+1} + R_{t+2} + R_{t+3}$,一般情况下我们寻求最大化预期收益。其中收益G_{t}被定义为奖励序列的某个特定函数。在最简单的情况下，回报是奖励的总和。

$$G_{t} \doteq R_{t+1} + R_{t+2} + R_{t+3} + ... + R_{T}$$

T是最后一步，这种方法在存在最终时间步骤的自然概念的应用中是有意义的。也就是说，当个体-环境交互自然地分解为子序列时，我们称之为情节。每个情节在称为终点状态的特殊状态结束。
我们需要的另一个概念是衰减因子。根据这种方法，个体尝试选择动作，以使其在未来接收的衰减的奖励的总和最大化。特别是它选择$A_{t}$来最大化预期的衰减回报。

$$G_{t} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... = \sum^{\infty}_{k=0} \gamma^{k}R_{t+k+1}$$

其中$\gamma$是参数，称为衰减因子。

衰减率决定了未来奖励的现值：未来收到的k个时间步骤的奖励价值仅为立即受到的$\gamma^{k-1}$倍。如果奖励是立即被接收的则是值得的。如果$\gamma=0$，个体是短视的，只关注最大化立即奖励：在这种情况下，其目标是学习如何选择$A_{t}$以使$R_{t+1}$最大化。如果每个个体的行为恰好只影响即时奖励，而不影响未来的奖励，那么短视个体可以通过单独最大化每个及时奖励来最大化。但一般来说，最大化立即奖励的行为可以减少对未来奖励的获取，从而减少回报。

## 情节和持续任务的统一符号
上一节中描述了两种强化学习任务，其中一种是个体-环境交互自然地分解为一系列单独的情节，称为**情节任务**；另一种则称为**连续任务**。本章将建立一种能够同时准确地谈论这两种情况的符号。

准确地描述情节性任务需要一些额外的符号。我们需要考虑一系列情节，每个情节都由有限的时间步骤序列组成。我们从零开始重新编号每个情节的时间步长，因此，我们不仅要参考时间t的状态表示$S_{t}$，而且参考在情节i和时间t的状态表示St，i（事实证明是不必的）

我们需要另一个约定来获得涵盖情节和持续任务的单一符号。在一种情况下，我们将收益定义为有限数量的项的和，而在另一种情况下，将收益定义为无限数量的项目。这两个可以通过考虑情节种植来统一，即进入一个特殊的吸收状态。该状态仅转换自身并且仅产生零奖励。例如，考虑状态转换图。
这里实心方块表示对应于情节结束的特殊吸收状态。从S0开始，我们得到奖励序列。总结这些，我们得到相同的回报，无论我们是否在前T个奖励求和还是在整个无限序列上求和。即是我们引入衰减因子，


## 策略和价值函数
几乎所有的强化学习算法都涉及估计状态（状态-动作对）的价值函数。它们估计个体在给定状态下执行给定动作的好坏程度。这里的“好坏程度”是根据未来的奖励来定义的，或者准确的说是预期回报方面。当然个体未来可能获得的回报取决于它将采取的行动，因此价值函数是根据特定的行为方式来定义的，称为策略。
形式上，策略是从状态到选择每个可能动作的概率的映射。如果个体在时间t遵循策略$\pi$，则$\pi(a|s)$是如果$S_{t}=s$则$A_{t}=a$的概率。像p一样，pi是个普通的函数。$\pi(a|s)$中间的|仅提醒它为了每个$s\in S$定义了$\alpha \in A(s)$的概率分布。强化学习方法制定了个体的策略如何因其经验结果而变化。


















