---
layout: post
title: 【强化学习导论】k臂赌博机问题(2)・$\epsilon$贪婪方法
categories: Reinforcement-Learning
---
## 行动估算价值的样本平均方法

估算行动价值并做出行动选择决策的方法统称为**行动价值方法**。一种方法是平均实际收到的奖励，如下所示：

$$Q_{t}(a)\doteq\frac{t之前采取a动作的奖励总和}{t之前采取a动作的次数}=\frac{\Sigma^{t-1}_{i=1}R_{i}\cdot\mathbb{1}_{A_{i}=a}}{\Sigma^{t-1}_{i=1}\cdot\mathbb{1}_{A_{i}=a}}$$

在$\mathbb{1}\_{condition}$中，如果条件为真，则为1，反之则为0。如果分母为零，则将$Q_{t}(a)$定义为某个默认值（例如0）。当分母无穷大时，根据大数定律，$Q_{t}(a)$收敛于$q_{\*}(a)$。  
此方法称为用于估计行动值的**样本平均方法**，当然这只是估算行动价值的一种方式。现在让我们继续使用这种简单的估算方法，并转向如何用估算选择行动的问题。  

## $\epsilon$贪婪方法
最简单的动作选择规则是选择具有最高估计值的动作之一，即上一章提到的贪婪动作。如果存在多个贪婪动作，则可以以任意方式在它们之间进行选择。我们把这个贪婪的动作选择方法写成：

$$A_{t}=\underset{a}{argmax} Q_{t}(a)$$

其中$\underset{a}{argmax}$表示随后的表达式最大时的动作$a$。贪婪行动选择总是利用当前的知识来最大化立即奖励，而不会花时间采取明显劣质的行动。一个简单的替代方案是在大多数情况下贪婪地行动，但每隔一段时间，以较小的概率$\epsilon$从具有相同概率的所有动作中随机选择。我们称之为$\epsilon$贪婪方法。
