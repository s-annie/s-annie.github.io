---
layout: post
title: 【RL】k臂赌博机(3)・10臂赌博机试验
categories: Reinforcement-Learning
---
## 试验目的
本试验的目的为粗略评估贪婪和$\epsilon$贪婪行动价值方法的相对有效性。这是一组2000个随机生成的$k$臂赌博机问题，$k=10$。对于每个赌博机问题，动作价值$$q_{\*}(a),a=1,...10$，根据具有均值为0和方差为1的正态（高斯）分布来选择。当应用于该问题的学习方法选择动作$A_{t}$时，从具有均值$q_{*}(A_{t})$和方差1的正态分布（在图中以灰色显示）中选择实际奖励$R_{t}$。

<img src="/assets/post/2021-8-10/1.png" class="center" width=50%>

## 试验结果
以下为贪婪方法和两个$\epsilon$方法的比较。  
上方图片显示了带有经验的预期奖励的增加。贪婪方法在开始时比其它方法改善略快，但最后在较低水平上稳定了下来。下方图片显示贪婪的方法
大约只在三分之一的任务中找到了最佳动作。  
另一方面，$\epsilon$贪婪的方法最终表现得更好，因为它们会继续探索并提高识别最佳动作的机会。$\epsilon=0.01$的改进更慢，但最终在关于两图的性能指标上会比$\epsilon=0.1$的方法更好。

## 试验小结
$\epsilon$方法优于贪婪方法的优势取决于任务。例如奖励方差较大，比如说是10而不是1，这种更嘈杂的奖励需要更多的探索才能找到最佳动作。再例如奖励方差为零，贪婪方法在尝试一次后就会找到每个动作的真实价值，这种情况下贪婪方法实际上可能表现最佳。  
当赌博机任务是非平稳[^1]的（即行动的真实价值随着时间而变化），这种情况下探索将是一个很大的优势，可以确保其中一个非贪婪动作没有变得更比贪婪动作更好。

[^1]:  非平稳性是强化学习中最常遇到的情况。即使基础任务是固定的和确定的，学习者也面临着一系列角色人物。随着学习的进行和各地的决策制定策略的变化，这些决策随着时间的推移而变化。强化学习需要在探索和利用之间取得平衡。