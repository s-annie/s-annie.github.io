---
layout: post
title: 【RL】k臂赌博机问题（1）探索与利用的冲突
categories: Reinforcement-Learning
---
### k臂赌博机问题描述
假设你可以反复面对$k$种不同的选择或行动。每次选择之后，你会收到一个数值奖励。该奖励取决于你选择的行动的固定概率分布。你的目标是在一段时间内最大化预期的总奖励。
### 使用此案例的目的
区分强化学习与其他类型学习的最重要特征是，它使用训练信息来**评估**所采取的行动，而不是通过给予正确的行动来**指导**。这两种反馈是截然不同的：评估反馈完全取决于所采取的行动，而指导反馈则与所采取的行动无关。
使用$k$臂赌博机问题可以在简化的环境中研究强化学习的评价方面。该问题不涉及学习如何在多种情况下行动，能够最清楚地看到评估反馈与指导反馈的不同。
### 详细内容
#### 动作价值
在这个问题中，$k$个动作的每一个都有预期的或平均的奖励，称为该动作的价值。我们在时间步骤$t$的动作选择表示为$A$，并将相应的奖励表示为$R_{t}$，对于任意动作$a$的价值，定义$q_{*}(a)$是$a$选择的预期奖励。  
$$q_{*}(a)\doteq\mathbb{E}[R_{t}|A_{t}=a]$$  s
假设你不确定地知道动作价值。我们将时间步骤$t$的动作$a$的估计价值表示为$Q_{t}(a)$。我们希望$Q_{t}(a)$接近$q_{*}(a)$。
#### 探索，利用与贪婪行为
在任何时间步骤中至少有一个其估计价值最大的动作，称为**贪婪行为**。当你选择估计值最大的动作时，我们会出你正在利用当前对动作价值的了解。相反，如果你选择了一个非常规动作，我们就说你正在探索，因为这可以提高对非常规动作价值的估计。利用是在一步中最大化预期的奖励的最好方法，但从长远来看，探索可能会产生更大的总回报。[^1]
平衡探索和利用的需要是强化学习中的挑战。在$k$臂赌博机这一章中提出了几种简单的平衡方法，并表明它们比总是利用的方法更好。

[^1]: 假设贪婪行为的价值是确定的，而其他一些动作估计价值几乎同样好，但具有很大的不确定性。不确定性使得这些其他行动中的至少一个实际上可能比贪婪行动更好，但并不知道哪一个。在短期内，奖励在探索期间较低，但从长远来看更好，因为在你发现更好的行动之后，可以多次利用它们。
