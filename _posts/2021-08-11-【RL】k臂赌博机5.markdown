---
layout: post
title: 【RL】k臂赌博机(5)・$\epsilon$贪婪方法扩展说明
categories: Reinforcement-Learning
---
## 非平稳问题
$\epsilon$贪婪方法中使用的**样本平均方法**适用于固定赌博机问题，然而非常不稳定的强化学习问题十分常见。在这种情况下，给予最近的奖励比长期的奖励更重要。最常用的方法之一是使用**常量步长参数**。

$$Q_{n+1}\doteq Q_{n}+\alpha(R_{n}-Q_{n})$$

步长$\alpha\in(0, 1]$是常数。$Q_{n+1}$是过去奖励和初始估计$Q_{1}$的加权平均值。

$$
\begin{aligned}
Q_{n+1}&=Q_{n}+\alpha(R_{n}-Q_{n}) \\
&=\alpha R_{n}+(1-\alpha)Q_{n} \\
&=\alpha R_{n}+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}] \\
&=\alpha R_{n}+(1-\alpha)\alpha R_{n-1}+ (1-\alpha)^{2}\alpha R_{n-2}+.....+ (1-\alpha)^{n-1}\alpha R_{1} + + (1-\alpha)^{n}Q_{1}\\
&=(1-\alpha)^{n}Q_{1} + \sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}R_{i}
\end{aligned}
$$

因为权重之和$(1-\alpha)^{n}+\sum^{n}_ {i=1}\alpha(1-\alpha)^{n-i}=1$，所以称之为加权平均值。给予奖励$R_{i}$的权重取决于之前有多少奖励。另外，$1-\alpha$小于1，因此给予$R_{i}$的权重随着介入奖励数量的增加而减少。

接下来，我们尝试让$\alpha_{n}(\alpha)$表示用于处理在第$n$次动作选择$a$之后收到的奖励的步长参数。选择$\alpha_{n}(\alpha)=\frac{1}{n}$导致样本平均方法，保证通过大数定律收敛到真实的行动价值。当然，不是所有选择都能保证收敛。随机逼近理论提供了确保收敛概率为1所需的条件。

$$\sum^{\infty}_{n=1}\alpha_{n}(\alpha)=\infty \\
\sum^{\infty}_{n=1}\alpha_{n}^{2}(\alpha)<\infty$$

第一个条件是保证步骤足够大，以最终克服任何初始条件或随机波动。第二个条件保证最终步骤变得足够小以确保收敛。对于样本平均方法，$\alpha_{n}(a)=\frac{1}{n}$都满足两个收敛条件，但恒定步长参数$\alpha_{n}(a)=n$的情况不满足第二个条件，表明估计值从未完全收敛，但是响应于最近收到的奖励而继续变化。这在非平稳环境中实际上是可取的，另外满足上述条件的步长参数序列通常非常缓慢地收敛或需要相当大的微调以获得令人满意的收敛速率。

## 乐观的初始值
无论是样本平均方法，还是使用常量步长参数的方法，在某中程度上都依赖于初始行动价值估计$Q_{1}(a)$。在统计语言中，这些方法的初始估计存在偏差。对于样本平均方法，一旦所有动作至少被选择一次，偏差就会消失；但对于具有常数$\alpha$的方法，偏差是永久性的，尽管随着时间的推移逐渐减少（参见公式1）。  
初始行动价值可以用作鼓励探索的简单方法。假设不把初始动作值设置为零，而设置为+5。在10臂赌博机试验中，预期奖励$q_{*}(a)$是从具有均值0和方差1的正态分布中选择的，因此+5的初始动作值非常乐观。随后无论最初选择哪种行为，奖励都低于起始估计。学习者转向其他行动，对其收到的奖励感到“失望”，因此即使一直选择我贪婪的行为，系统也会进行大量的探索。

下图显示了分别使用$Q_{1}(a)=+5$, $Q_{1}(a)=0$的贪婪方法的10臂赌博机试验性能。最初，乐观方法表现更差，因为它探索更多；但最终它表现更好，因为它的探索随着时间推移而减少。

我们称这种技术为鼓励探索**乐观的初始值**。这是一个简单的技巧，对于静止问题非常有效，但它远不是一个鼓励探索的普遍方法。例如，它不适合非平稳问题，因为它的驱动对于探索本质上是暂时的。如果任务发生变化，再次需要探索，这种方法无法帮助。

实际上，任何以特殊方式关注初始条件的方法都不可能对一般的非平稳情况有所帮助。时间的开始只出现一次，因此我们不应该过分关注它。这种批评也适用于样本平均方法。然而，这些方法十分简单。其中之一，或者他们的一些简单组合，在实践中通常是足够的。
