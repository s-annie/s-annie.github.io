---
layout: post
title: 【强化学习导论】k臂赌博机问题(4)・样本平均方法的增量实现
categories: Reinforcement-Learning
---
[k臂赌博机(2)・$\epsilon$贪婪方法](https://s-annie.github.io/reinforcement-learning/2021/08/10/RL-k臂赌博机2.html)中提到过行动价值估算所用到的样本平均方法，本章讨论在具有恒定内存和恒定的时间步长的条件下如何计算平均值。  
为了简化表示法，我们专注于单一动作。$R_{i}$表示在第$i$次选择此动作之后收到的奖励，$Q_{n}$次之后其动作价值估计可以用以下形式来表示。
$$Q_{n}\doteq\frac{R_{1}+R_{2}+...+R_{n-1}}{n-1}$$

如果使用此公式的话，需要维护所有奖励的记录，然后在需要估计价值时执行该计算。但是如果这样做，随着时间的推移，内存和计算要求会随着更多的奖励而增长。  
这个问题可以通过处理每个新奖励来解决。以下为用于更新平均值的增量公式。给定$Q_{n}$和第$n$个奖励$R_{n}$，可以计算所有$n$个奖励的新平均值。

$$
\begin{aligned}
Q_{n+1}&=\frac{1}{n}\sum^{n}_{i=1}R_{i} \\
&=\frac{1}{n}(R_{n}+\sum^{n-1}_{i=1}R_{i}) \\
&=\frac{1}{n}(R_{n}+(n-1)\frac{1}{n-1}\sum^{n-1}_{i=1}R_{i}) \\
&=\frac{1}{n}(R_{n}+(n-1)Q_{n}) \\
&=\frac{1}{n}(R_{n}+nQ_{n}-Q_{n}) \\
&=Q_{n}+\frac{1}{n}(R_{n}-Q_{n}) \\
\end{aligned}
$$

文字描述：**新估计=旧估计+步长[目标-旧估计]**。这个更新规则今后也会经常出现。
