---
layout: post
title:【RL】k臂赌博机(7)・赌博机问题的梯度算法
categories: Reinforcement-Learning
---

在这个算法中，我们考虑为每个动作$a$学习数字偏好，表示为$H_{t}(a)$。偏好越大，采取行动的次数越多，但偏好在奖励方面没有解释。只有一种行为相对于另一种行为的相对偏好才是重要的。如果我们将1000添加到所有动作首选项，则对动作概率没有影响。动作概率是根据soft-max分布（如Gibbs或Boltzman分布）确定的。如下所示。

$$P_{r}\{A_{t}=a\}\doteq\frac{e^{H_{t}(a)}}{\sum^{k}_{b=1}e^{H_{t}(b)}}\doteq\pi_{t}(a)$$

这里我们引入了一个有用的符号$\pi_{t}(a)$，表示在时间t采取行动的概率。最初，所有动作偏好都是相同的(对于所有$a$，$H_{1}(a)=0$)，使得所有动作具有相同的被选择概率。

在每个步骤中，在选择动作$A_{t}$并接受奖励$R_{t}$之后，动作偏好通过以下方式更新。

$$H_{t+1}(A_{t})\doteq H_{t}(A_{t})+\alpha(R_{t}-\overline R_{t})(1-\pi_{t}(A_{t})) \\
H_{t+1}(a)\doteq H_{t}(a)-\alpha(R_{t}-\overline R_{t})\pi_{t}(a),a \neq A_{t}$$

其中$\alpha>0$是步长参数，$\overline R_{t} \in(R)$是所有奖励的平均值，包括时间$t$。$\overline R_{t}$作为比较奖励的基线，如果奖励高于基线，那么将来获取$A_{t}$的概率增加；如果奖励低于基线，则概率降低。
